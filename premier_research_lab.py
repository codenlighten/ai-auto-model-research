"""
Premier AI Research Lab - Real Experiments
Focus: Energy-efficient training and inference innovations

This script runs ACTUAL ML experiments that produce publishable results
"""

import sys
import os
from pathlib import Path

sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from ai_research_team import AIResearchTeam
from research_tracker import ResearchDatabase

# Premier research questions for breakthrough innovations
RESEARCH_EXPERIMENTS = {
    
    "quantization_breakthrough": """
Discover optimal quantization strategy for transformers:

Test dynamic INT8 quantization vs static quantization vs mixed precision (FP16)
on a small transformer model (2 layers, 256 hidden dim, 4 heads).

Train for 500 steps on synthetic data.

Measure:
1. Model size reduction (compression ratio)
2. Inference speedup (ms per forward pass)
3. Accuracy retention (% of FP32 baseline)
4. Training time difference
5. Memory footprint (MB)

Create a complete experiment that trains 3 models (FP32 baseline, INT8, FP16)
and compares all metrics. Print detailed comparison table.

MUST include actual training loop with loss tracking and final results output.
""",

    "pruning_innovation": """
Develop magnitude-based pruning schedule for CNNs:

Test 3 pruning ratios (30%, 50%, 70%) on a small CNN (3 conv layers).
Train on MNIST-like synthetic data for 300 steps.

Implement:
1. Baseline unpruned model
2. One-shot magnitude pruning at each ratio
3. Gradual pruning over training

Measure:
1. Accuracy vs pruning ratio curve
2. Inference speedup from parameter reduction
3. Model size savings (MB)
4. Training stability (loss curve smoothness)

Output: Which pruning strategy gives best accuracy/efficiency tradeoff?

Code must train all models and print comparison table with metrics.
""",

    "efficient_optimizer": """
Compare modern optimizers for energy-efficient training:

Test Adam vs AdamW vs SGD with momentum on small MLP (3 layers, 256 hidden).
Train each for 200 steps with same learning rate.

Track:
1. Convergence speed (steps to reach loss threshold)
2. Final loss achieved
3. Training time (seconds)
4. Memory usage per optimizer
5. Gradient update stability

Generate real training curves and determine:
- Which optimizer converges fastest?
- Which uses least memory?
- Best accuracy/speed tradeoff?

Must include complete training loop with progress printing.
""",

    "micro_architecture": """
Design ultra-efficient micro-transformer architecture:

Create 3 transformer variants:
1. Standard (Multi-Head Attention, FFN)
2. Efficient (Linear attention, smaller FFN)
3. Tiny (Single-head, shared weights)

Each with ~100K parameters, train for 400 steps.

Compare:
1. Parameter count
2. FLOPs per forward pass
3. Training time
4. Inference latency
5. Final loss

Goal: Find architecture that's 2x faster with <10% accuracy loss.

Code must implement all 3 models and train them with metrics.
""",

    "mixed_precision_training": """
Benchmark Automatic Mixed Precision (AMP) training efficiency:

Train same model (simple CNN) with:
1. FP32 (baseline)
2. FP16 with AMP
3. BF16 (if available)

300 training steps each.

Measure:
1. Training speedup (time per epoch)
2. Memory reduction (peak MB)
3. Loss convergence (final values)
4. Numerical stability (gradient norms)
5. Energy consumption estimate

Determine: Is AMP always beneficial? When does it help most?

Must include actual training with torch.cuda.amp if available, else CPU.
""",

    "knowledge_distillation": """
Implement teacher-student knowledge distillation:

Teacher: 4-layer MLP (784‚Üí512‚Üí256‚Üí128‚Üí10)
Student: 2-layer MLP (784‚Üí128‚Üí10)

Train student with:
1. Hard labels only (baseline)
2. Soft labels from teacher (T=3)
3. Combined hard + soft (alpha=0.5)

200 steps each on synthetic MNIST-like data.

Track:
1. Student accuracy vs teacher accuracy
2. Student model size vs teacher
3. Inference speedup
4. Training time difference

Show which distillation method works best.

Code must train all models and output comparison.
""",

    "gradient_checkpointing": """
Test gradient checkpointing for memory-efficient training:

Deep MLP (10 layers) trained with and without checkpointing.
Train for 200 steps.

Measure:
1. Peak memory usage (MB)
2. Training time overhead
3. Final loss comparison
4. Memory vs speed tradeoff

Determine: Is checkpointing worth it for smaller models?

Must include actual implementation with torch.utils.checkpoint.
""",

    "batch_size_optimization": """
Find optimal batch size for training efficiency:

Test batch sizes: [8, 16, 32, 64, 128]
Same model, same total training steps (1000).

Track for each:
1. Time per step (ms)
2. Throughput (samples/sec)
3. Memory usage (MB)
4. Final loss achieved
5. Total training time

Goal: Find sweet spot that maximizes throughput without OOM.

Code must train with all batch sizes and compare results.
""",

    # FOCUSED HIGH-QUALITY EXPERIMENTS (Designed for 7+/10 breakthrough)
    "pruning_analysis": """
Comprehensive pruning study with fine-grained analysis:

Test pruning ratios: 10%, 20%, 30%, 40%, 50%, 60%, 70%
Train baseline SimpleCNN for 10 epochs on 2000 samples
For each pruning ratio:
  - Apply magnitude pruning  
  - Fine-tune for 5 epochs
  - Measure accuracy, size, inference time

Discover:
1. Optimal pruning ratio (best accuracy/size tradeoff)
2. Critical pruning threshold (where accuracy drops >10%)
3. Inference speedup curve
4. Model compression ratio

Output: Complete comparison table + visualization-ready data + insights on pruning sweet spot.
Code must be clean, well-documented, and produce publication-quality results.
Use ml_utils: SimpleCNN, create_synthetic_mnist_images, prune_model, compare_models

CRITICAL - END WITH INSIGHTS SECTION:
After printing all metrics, add a "RESEARCH INSIGHTS" section with:
1. Key Finding: One sentence breakthrough discovery
2. Practical Recommendation: Actionable advice for practitioners
3. Surprising Result: What was unexpected?
4. Future Direction: What to explore next
Make insights clear, specific, and valuable.
""",

    "learning_rate_discovery": """
Systematic learning rate optimization study:

Test learning rates: [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1]
Train SimpleMLP on 2000 samples for 15 epochs each
Track for each LR:
  - Final accuracy
  - Training stability (loss variance)
  - Convergence speed (epochs to 90% of final performance)
  - Overfitting tendency

Discover:
1. Optimal LR for fast convergence
2. LR ranges that cause instability
3. Sweet spot for accuracy vs speed
4. Relationship between LR and generalization

Output: Detailed analysis with convergence curves, stability metrics, and actionable recommendations.
Code must include proper validation split and statistical analysis.
Use ml_utils: SimpleMLP, create_synthetic_mnist, train_simple_classifier, evaluate_accuracy

CRITICAL - END WITH INSIGHTS SECTION:
After printing all metrics, add a "RESEARCH INSIGHTS" section with:
1. Key Finding: One sentence breakthrough discovery about learning rate
2. Practical Recommendation: Best LR range for this architecture
3. Surprising Result: Non-obvious pattern in convergence behavior
4. Future Direction: How to extend this analysis
Make insights clear, specific, and valuable for ML practitioners.
""",

    "architecture_depth_study": """
Study impact of network depth on performance:

Create MLPs with depths: 2, 3, 4, 5, 6, 7, 8 layers
Keep total parameters ~constant (~130K) by adjusting width
Train each for 12 epochs on 2000 samples

Measure:
1. Training time per epoch
2. Final validation accuracy
3. Overfitting gap (train vs val accuracy)
4. Inference latency
5. Parameter efficiency

Discover:
1. Optimal depth for this problem
2. Point of diminishing returns
3. Depth vs overfitting relationship  
4. Depth vs training efficiency

Output: Complete analysis showing depth sweet spot with statistical significance testing.
Code must implement all depths cleanly using PyTorch nn.Sequential and ml_utils helpers.
Use ml_utils: create_synthetic_mnist, get_model_size, measure_inference_time, evaluate_accuracy

CRITICAL - END WITH INSIGHTS SECTION:
After printing all metrics, add a "RESEARCH INSIGHTS" section with:
1. Key Finding: Optimal depth and why it matters
2. Practical Recommendation: Depth guidelines for similar problems
3. Surprising Result: Unexpected relationship between depth and performance
4. Future Direction: Next architecture experiments to try
Make insights actionable and backed by the data.
"""
}


def run_research_experiment(experiment_name):
    """Run a specific research experiment"""
    
    if experiment_name not in RESEARCH_EXPERIMENTS:
        print(f"‚ùå Unknown experiment: {experiment_name}")
        print(f"\nAvailable experiments:")
        for name in RESEARCH_EXPERIMENTS.keys():
            print(f"  - {name}")
        return
    
    research_goal = RESEARCH_EXPERIMENTS[experiment_name]
    
    print("\n" + "="*80)
    print(f"üî¨ PREMIER AI RESEARCH LAB")
    print(f"üìä Experiment: {experiment_name.replace('_', ' ').title()}")
    print("="*80)
    
    team = AIResearchTeam()
    
    # Verify agents
    try:
        agents = team.client.get_my_agents()
        print(f"‚úÖ Research team ready: {agents.get('count', 0)} specialist agents")
    except Exception as e:
        print(f"‚ö†Ô∏è  Setting up research team...")
        team.setup_team()
    
    print(f"\nüéØ Research Question:")
    print(research_goal.strip()[:200] + "...")
    print()
    
    # Run the experiment sprint
    sprint_result = team.run_sprint(research_goal.strip())
    
    # Track in database
    db = ResearchDatabase()
    exp_id = db.add_experiment(sprint_result)
    print(f"\nüíæ Experiment tracked: {exp_id}")
    db.close()
    
    # Extract results
    implementation = sprint_result.get("phases", {}).get("implementation", {})
    execution = implementation.get("execution", {})
    validation = sprint_result.get("phases", {}).get("validation", {})
    
    print("\n" + "="*80)
    print("üìä EXPERIMENT RESULTS")
    print("="*80)
    
    if execution.get("success"):
        print("‚úÖ Experiment executed successfully!\n")
        
        output = execution.get("stdout", "")
        if output:
            print("üìà Experimental Output:")
            print("-" * 80)
            print(output)
            print("-" * 80)
        
        # Check for results file
        sprint_id = sprint_result.get("sprint_id")
        results_dir = Path(f"results/sprint_{sprint_id}")
        
        # Look for JSON results
        json_files = list(results_dir.glob("*.json"))
        if json_files and json_files[0].name != "sprint_summary.json":
            print(f"\nüìä Results file: {json_files[0]}")
    else:
        print("‚ùå Experiment failed\n")
        stderr = execution.get("stderr", "")
        if stderr:
            print("Error output:")
            print(stderr[:500])
    
    # Validation analysis
    if validation.get("analysis"):
        print(f"\nüîç Scientific Validation:")
        print("-" * 80)
        analysis = validation.get("analysis", "")
        # Show first key points
        lines = analysis.split('\n')
        for line in lines[:20]:  # First 20 lines
            if line.strip():
                print(line)
    
    print("\n" + "="*80)
    print(f"üìÅ Full results: results/sprint_{sprint_result.get('sprint_id')}/")
    print("="*80 + "\n")
    
    return sprint_result


def list_experiments():
    """List all available research experiments"""
    print("\nüî¨ PREMIER AI RESEARCH LAB - Available Experiments\n")
    print("="*80)
    
    for i, (name, description) in enumerate(RESEARCH_EXPERIMENTS.items(), 1):
        print(f"\n{i}. {name.replace('_', ' ').upper()}")
        print(f"   {description.split('Measure:')[0].strip()[:150]}...")
    
    print("\n" + "="*80)
    print(f"\nüí° Run with: python premier_research_lab.py <experiment_name>")
    print(f"   Example: python premier_research_lab.py quantization_breakthrough\n")


def main():
    """Main entry point"""
    
    if len(sys.argv) < 2:
        list_experiments()
        return
    
    experiment_name = sys.argv[1]
    
    if experiment_name == "list":
        list_experiments()
    else:
        run_research_experiment(experiment_name)


if __name__ == "__main__":
    main()
